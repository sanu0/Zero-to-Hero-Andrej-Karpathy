{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1cezXR+5l86jLeS05BZhh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanu0/Zero-to-Hero-Andrej-Karpathy/blob/main/lec_1_micrograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Micrograd is a tiny Autograd (Automatic gradient) . Implements  backpropagation (reverse-mode autodiff) over a dynamically built DAG and a small neural networks library on top of it with a PyTorch-like API. Both are tiny, with about 100 and 50 lines of code respectively. The DAG only operates over scaler values, so e.g. we chop up each neuron into all of its individual tiny adds and multiples. However, this is enough to build up entire dep neural network nets doing binary classification, as the demo notebook shows. Potentially useful for the educational purpose."
      ],
      "metadata": {
        "id": "WeR1SVO9FPJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install micrograd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIxfHwUkFTAr",
        "outputId": "f04e26cb-8471-497d-e06b-07ec589a36da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: micrograd in /usr/local/lib/python3.12/dist-packages (0.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from micrograd.engine import Value\n",
        "\n",
        "a = Value(-4.0)\n",
        "b = Value(2.0)\n",
        "c = a + b\n",
        "d = a * b + b**3\n",
        "c += c + 1\n",
        "c += 1 + c + (-a)\n",
        "d += d * 2 + (b + a).relu()\n",
        "d += 3 * d + (b - a).relu()\n",
        "e = c - d\n",
        "f = e**2\n",
        "g = f / 2.0\n",
        "g += 10\n",
        "\n",
        "print(f\"{g.data:.4f}\") #prints 24.7041, the outcome of this forward pass\n",
        "g.backward()\n",
        "print(f\"{a.grad:.4f}\") #prints 138.8338, numerical value of dg/da\n",
        "print(f\"{b.grad:.4f}\") #prints 645.0809, numerical value of dg/db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVh8j4lSFgqi",
        "outputId": "0f793a9b-058a-442f-d1df-7812ec4ed2af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.5000\n",
            "140.0000\n",
            "651.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so what we are doing here is that not just checking the value of “g”, which is straight forward forward pass. We can also call g.backward(), which in turn gives you the gradient of g with respect to middle variables like c and d and e and f. It also gives you the derivatives of g w.r.t inputs a and b.\n",
        "So,  what derivatives shows is how the last var which is our output which is g is dependent on the intermediate as well as the input variables.  So derivatives is telling us how inputs a and b and even intermediates are effecting “g”.\n",
        "\n",
        "So if the dg/da = 138, means it is gonna tell us how g will repond based on the tiny changes on a, here the deriavtive is 138 meaning that when we are doing small nudge in a then it will effect g 138 times of that nudge in positive direction.\n",
        "\n",
        "Note that whatever we are doing in the micrograd, it is done on the scaler and it is the most simplistic view of the newral network and the maths that are involved behind the neural network. Now in the real world we are using the libraries like tensorflow and pytorch, and they are using tensors (arrays and matrices) and we are doing that coz we wanna use the parallelism of the hardware so that all these can be done simultanously. But none of the math changes and all these are done just for the sake of efficiency."
      ],
      "metadata": {
        "id": "xlibt8sgGDpu"
      }
    }
  ]
}